import json
import logging
import os
import pathlib
import subprocess
from collections import defaultdict
from datetime import datetime

from ryu.lib import hub

CURRENT_PATH = pathlib.Path().absolute()
logger = logging.getLogger(__name__)
logger.setLevel(level=logging.WARNING)

class FlowMonitor(object):

    def __init__(self, report_folder, watch_generated_flows, flow_statistics_save_period, *args, **kwargs):
        super(FlowMonitor, self).__init__(*args, **kwargs)
        self.flows = defaultdict()

        self.statistics = defaultdict()
        self.statistics["flows"] = defaultdict()
        self.statistics["created-flow-count"] = 0
        self.statistics["removed-flow-count"] = 0

        self.report_folder = report_folder
        self.watch_generated_flows = watch_generated_flows  # If flows generated by this class is reported, It is used to test
        self.flow_statistics_save_period = flow_statistics_save_period  # if watch_generated_flows True, defines flow statistics save period

        if watch_generated_flows:
            logger.warning("Generated_flows will be watched !!! All flows statistics will be saved.")
        #if self.watch_generated_flows:
        #    hub.spawn_after(5, self._save_statistics_periodically)

    def add_to_flow_list(self, dpid, flow_id, match, actions, priority, idle_timeout, hard_timeout):
        if self.watch_generated_flows:
            if dpid not in self.statistics["flows"]:
                self.statistics["flows"][dpid] = {}

            self.statistics["flows"][dpid][flow_id] = {"match": match,
                                                       "actions": actions,
                                                       "priority": priority,
                                                       "idle_timeout": idle_timeout,
                                                       "hard_timeout": hard_timeout,
                                                       "created": datetime.now().timestamp(),
                                                       "datapath_id": dpid
                                                       }
            self.statistics["created-flow-count"] = self.statistics["created-flow-count"] + 1

    def _save_statistics_periodically(self):
        sleep_time = self.flow_statistics_save_period
        while True:
            logger.warning(f'{datetime.now()} - SDN Contoller statistics will be saved after  {sleep_time} sec. into {self.report_folder}')
            hub.sleep(sleep_time)
            self._save_statistics()
            logger.warning(f'{datetime.now()} - SDN Contoller statistics are saved.')

    def _save_statistics(self):

        report_path = os.path.join(CURRENT_PATH, "reports", self.report_folder)

        pathlib.Path(report_path).mkdir(parents=True, exist_ok=True)

        file_name = "%s-controller_flows.json" % (self.report_folder)

        file_path = os.path.join(report_path, file_name)
        comparator = lambda o: o.__str__() if isinstance(o, object) else None
        with open(file_path, 'w') as outfile:
            json.dump(self.statistics, outfile, default=comparator)
        subprocess.call(['chmod', "-R", '0777', report_path])



    def flow_stats_reply_handler(self, ev):
        body = ev.msg.body
        datapath_id = ev.msg.datapath.id

        max_packet_count = 0
        eth_src = -1
        eth_dst = -1
        in_port = -1

        initial = True
        for flow in body:
            if max_packet_count < flow.packet_count:
                max_packet_count = flow.packet_count
            if initial:
                if "in_port" in flow.match:
                    in_port = flow.match["in_port"]

                if "eth_src" in flow.match:
                    eth_src = flow.match["eth_src"]

                if "eth_src" in flow.match:
                    eth_dst = flow.match["eth_dst"]
                initial = False

    def flow_removed_handler(self, ev):
        msg = ev.msg
        dp = msg.datapath

        if dp.id in self.flows:
            if msg.cookie in self.flows[dp.id]:
                manager = self.flows[dp.id][msg.cookie]
                manager[1].flow_removed(msg)
                del self.flows[dp.id][msg.cookie]

    def flow_removed(self, msg):
        if self.watch_generated_flows:
            if msg.datapath.id in self.statistics["flows"]:
                if msg.cookie in self.statistics["flows"][msg.datapath.id]:
                    self.statistics["removed-flow-count"] = self.statistics["removed-flow-count"] + 1
                    stats = self.statistics["flows"][msg.datapath.id][msg.cookie]
                    stats["removed_time"] = datetime.now().timestamp()
                    stats["packet_count"] = msg.packet_count
                    stats["byte_count"] = msg.byte_count
                    stats["duration_sec"] = msg.duration_sec
                    stats["duration_nsec"] = msg.duration_nsec
                    stats["hard_timeout"] = msg.hard_timeout
                    stats["idle_timeout"] = msg.idle_timeout
                    stats["priority"] = msg.priority
                    stats["reason"] = msg.reason
                    stats["table_id"] = msg.table_id
